{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b400ac",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a470ac",
   "metadata": {},
   "source": [
    "# Single Agent and Enviroment without Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b66325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class TrainSchedulingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TrainSchedulingEnv, self).__init__()\n",
    "\n",
    "        # Stations and their routes (in minutes)\n",
    "        self.stations = [\"Chennai\", \"Madurai\", \"Coimbatore\", \"Tiruchirappalli\", \"Salem\"]\n",
    "        self.routes = {\n",
    "            \"Chennai\": {\"Madurai\": 420, \"Coimbatore\": 480, \"Tiruchirappalli\": 330},\n",
    "            \"Madurai\": {\"Chennai\": 420, \"Tiruchirappalli\": 120, \"Coimbatore\": 240},\n",
    "            \"Coimbatore\": {\"Chennai\": 480, \"Madurai\": 240, \"Salem\": 210},\n",
    "            \"Tiruchirappalli\": {\"Chennai\": 330, \"Madurai\": 120, \"Salem\": 150},\n",
    "            \"Salem\": {\"Coimbatore\": 210, \"Tiruchirappalli\": 150}\n",
    "        }\n",
    "        self.num_stations = len(self.stations)\n",
    "\n",
    "        # Train details\n",
    "        self.num_trains = 5\n",
    "        self.max_tracks_per_station = 3\n",
    "\n",
    "        # Weather and maintenance\n",
    "        self.weather_conditions = {route: np.random.uniform(0.8, 1.2) for route in self.routes.keys()}\n",
    "        self.maintenance_schedule = {route: np.random.choice([0, 1], p=[0.9, 0.1]) for route in self.routes.keys()}  # 10% chance of maintenance\n",
    "\n",
    "        # Passenger demand (randomized at reset)\n",
    "        self.passenger_demand = None\n",
    "\n",
    "        # Action and state spaces\n",
    "        self.action_space = spaces.MultiDiscrete([self.num_stations] * self.num_trains)  # Next station for each train\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(self.num_trains, 6), dtype=np.float32\n",
    "        )  # [current_station, delay, destination, progress, weather, maintenance]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_states = np.zeros((self.num_trains, 6))\n",
    "        self.passenger_demand = np.random.randint(50, 200, size=(self.num_stations, self.num_stations))\n",
    "        for train_id in range(self.num_trains):\n",
    "            self.train_states[train_id, 0] = np.random.randint(0, self.num_stations)  # Random starting station\n",
    "            self.train_states[train_id, 2] = np.random.randint(0, self.num_stations)  # Random destination\n",
    "        return self.train_states\n",
    "\n",
    "    def step(self, actions):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        for train_id, next_station_index in enumerate(actions):\n",
    "            current_station_index = int(self.train_states[train_id, 0])\n",
    "            destination_station_index = int(self.train_states[train_id, 2])\n",
    "            current_station = self.stations[current_station_index]\n",
    "            next_station = self.stations[next_station_index]\n",
    "\n",
    "            # Check if route exists\n",
    "            if next_station not in self.routes[current_station]:\n",
    "                reward -= 50  # Heavy penalty for invalid route choice\n",
    "                continue\n",
    "\n",
    "            # Travel time, weather, and maintenance impact\n",
    "            travel_time = self.routes[current_station][next_station]\n",
    "            weather_impact = self.weather_conditions[current_station]\n",
    "            maintenance = self.maintenance_schedule[current_station]\n",
    "\n",
    "            if maintenance:\n",
    "                reward -= 30  # Penalty for choosing a route under maintenance\n",
    "                continue\n",
    "\n",
    "            adjusted_travel_time = travel_time * weather_impact\n",
    "            self.train_states[train_id, 1] += adjusted_travel_time  # Add delay\n",
    "            self.train_states[train_id, 0] = next_station_index  # Update current station\n",
    "\n",
    "            # Reward based on passenger satisfaction\n",
    "            passengers_satisfied = self.passenger_demand[current_station_index, destination_station_index]\n",
    "            reward += passengers_satisfied * 2 - adjusted_travel_time  # Positive reward for satisfying demand and minimizing delay\n",
    "\n",
    "            # Check if destination reached\n",
    "            if current_station_index == destination_station_index:\n",
    "                reward += 100  # Large reward for reaching destination\n",
    "                done = True  # Simulation ends when a train reaches its destination\n",
    "\n",
    "        return self.train_states, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Train States: {self.train_states}\")\n",
    "        for train_id, train_state in enumerate(self.train_states):\n",
    "            current_station = self.stations[int(train_state[0])]\n",
    "            destination_station = self.stations[int(train_state[2])]\n",
    "            print(\n",
    "                f\"Train {train_id}: Current Station={current_station}, Destination={destination_station}, \"\n",
    "                f\"Delay={train_state[1]:.2f} minutes\"\n",
    "            )\n",
    "\n",
    "# Create environment\n",
    "env = TrainSchedulingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Wrap the environment\n",
    "env = make_vec_env(lambda: TrainSchedulingEnv(), n_envs=1)\n",
    "\n",
    "# Train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000)\n",
    "\n",
    "# Test the model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a9500",
   "metadata": {},
   "source": [
    "## Even tried to simulate realworld train paths of india to check whether this solution works but it fails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import time  # To add delay for visualization\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('Train_details_22122017.csv', dtype={'Distance': str})\n",
    "\n",
    "# Convert the 'Distance' column to numeric, coercing errors into NaN\n",
    "train_data['Distance'] = pd.to_numeric(train_data['Distance'], errors='coerce')\n",
    "\n",
    "# Select data for multiple trains (e.g., Train No 107, 108, 109)\n",
    "train_numbers = [107, 108, 109]\n",
    "train_subset = train_data\n",
    "\n",
    "# Create a graph with NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add stations as nodes\n",
    "for _, row in train_subset.iterrows():\n",
    "    G.add_node(row[\"Station Name\"], station_code=row[\"Station Code\"])\n",
    "\n",
    "# Add routes (edges) between stations for each train\n",
    "for i in range(1, len(train_subset)):\n",
    "    source = train_subset.iloc[i-1]\n",
    "    target = train_subset.iloc[i]\n",
    "\n",
    "    if pd.notna(source[\"Distance\"]) and pd.notna(target[\"Distance\"]):\n",
    "        G.add_edge(\n",
    "            source[\"Station Name\"],\n",
    "            target[\"Station Name\"],\n",
    "            distance=target[\"Distance\"] - source[\"Distance\"],\n",
    "            departure_time=target[\"Departure Time\"],\n",
    "            arrival_time=source[\"Arrival time\"]\n",
    "        )\n",
    "\n",
    "# Check the graph\n",
    "print(f\"Number of nodes (stations) in the graph: {len(G.nodes())}\")\n",
    "print(f\"Number of edges (routes) in the graph: {len(G.edges())}\")\n",
    "\n",
    "# Ensure that there are nodes to create the action space\n",
    "if len(G.nodes()) == 0:\n",
    "    raise ValueError(\"Graph has no nodes. Ensure the dataset contains valid station data.\")\n",
    "\n",
    "# Define the environment\n",
    "class TrainMovementEnv(gym.Env):\n",
    "    def __init__(self, graph):\n",
    "        super(TrainMovementEnv, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.stations = list(graph.nodes())  # List of station names\n",
    "        self.current_station = 0  # Start at the first station\n",
    "        self.action_space = spaces.Discrete(len(self.stations))  # Actions: move to a station\n",
    "        self.observation_space = spaces.Discrete(len(self.stations))  # Observations: current station\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_station = 0  # Reset to the first station\n",
    "        return self.current_station\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= len(self.stations):\n",
    "            return self.current_station, -1, False, {}\n",
    "\n",
    "        # Move to the selected station\n",
    "        next_station = self.stations[action]\n",
    "        if next_station in self.graph[self.stations[self.current_station]]:\n",
    "            self.current_station = action\n",
    "            return self.current_station, 0, False, {}  # No reward, still moving\n",
    "        else:\n",
    "            return self.current_station, -1, False, {}  # Invalid move\n",
    "\n",
    "    def render(self):\n",
    "        # Plot the graph with the current station highlighted\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pos = nx.spring_layout(self.graph, seed=42)  # Use a layout for better positioning\n",
    "        nx.draw(self.graph, pos, with_labels=True, node_size=5000, node_color='skyblue', font_size=10)\n",
    "\n",
    "        # Highlight the current station in a different color\n",
    "        current_station = self.stations[self.current_station]\n",
    "        nx.draw_networkx_nodes(self.graph, pos, nodelist=[current_station], node_size=5000, node_color='red')\n",
    "\n",
    "        plt.title(f\"Train is at {current_station}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Adding a small delay to visualize the movement\n",
    "        time.sleep(1)\n",
    "\n",
    "# Initialize environment with the graph of train stations\n",
    "env = TrainMovementEnv(G)\n",
    "\n",
    "# Example of simulating one episode\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Choose a random action (move to a random station)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()  # Render the current state after each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Example: Create a simple graph\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"Mumbai\", \"Delhi\", weight=1000)\n",
    "G.add_edge(\"Delhi\", \"Kolkata\", weight=1500)\n",
    "G.add_edge(\"Mumbai\", \"Chennai\", weight=1300)\n",
    "\n",
    "# Plot with NetworkX\n",
    "pos = nx.spring_layout(G)  # You can replace with geospatial coordinates\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709dfe6",
   "metadata": {},
   "source": [
    "# Mutli Agent and Enviroment with Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63443ad7",
   "metadata": {},
   "source": [
    "## First deliverable, Reinforcement learning model for train scheduling optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da28e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RailEnvironment:\n",
    "    def __init__(self, grid_size=(10, 10), n_agents=3):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_agents = n_agents\n",
    "        self.grid = np.zeros(grid_size)  # 0: empty, 1: station, 2: track\n",
    "        self.agents = []\n",
    "        self._initialize_environment()\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        # Place stations randomly\n",
    "        num_stations = 5\n",
    "        self.stations = [tuple(np.random.randint(0, self.grid_size[i], size=num_stations)) for i in range(2)]\n",
    "        for x, y in zip(self.stations[0], self.stations[1]):\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Initialize agents\n",
    "        for i in range(self.n_agents):\n",
    "            start = (random.choice(self.stations[0]), random.choice(self.stations[1]))\n",
    "            target = (random.choice(self.stations[0]), random.choice(self.stations[1]))\n",
    "            while target == start:\n",
    "                target = (random.choice(self.stations[0]), random.choice(self.stations[1]))\n",
    "            self.agents.append({\n",
    "                \"id\": i,\n",
    "                \"start\": start,\n",
    "                \"target\": target,\n",
    "                \"position\": start,\n",
    "                \"done\": False,\n",
    "                \"reward\": 0\n",
    "            })\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = {}\n",
    "        done = True\n",
    "        for agent, action in zip(self.agents, actions):\n",
    "            if agent[\"done\"]:\n",
    "                rewards[agent[\"id\"]] = 0\n",
    "                continue\n",
    "\n",
    "            # Compute new position\n",
    "            new_position = self._move(agent[\"position\"], action)\n",
    "            if not self._is_valid(new_position):\n",
    "                rewards[agent[\"id\"]] = -5  # Invalid move penalty\n",
    "                continue\n",
    "\n",
    "            # Update position and check if target is reached\n",
    "            agent[\"position\"] = new_position\n",
    "            if new_position == agent[\"target\"]:\n",
    "                agent[\"done\"] = True\n",
    "                rewards[agent[\"id\"]] = 10  # Reaching target reward\n",
    "            else:\n",
    "                rewards[agent[\"id\"]] = -1  # Step penalty\n",
    "\n",
    "            done = done and agent[\"done\"]\n",
    "\n",
    "        return self.grid, rewards, done\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        moves = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),   # Right\n",
    "            4: (0, 0)    # Wait\n",
    "        }\n",
    "        return (position[0] + moves[action][0], position[1] + moves[action][1])\n",
    "\n",
    "    def _is_valid(self, position):\n",
    "        return (0 <= position[0] < self.grid_size[0] and\n",
    "                0 <= position[1] < self.grid_size[1] and\n",
    "                self.grid[position] != 2)  # No collision with other tracks\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__(self.grid_size, self.n_agents)\n",
    "        return self.grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57d06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2a24b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: fsspec, torch\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\convert-caffe2-to-onnx.exe' -> 'C:\\\\Python311\\\\Scripts\\\\convert-caffe2-to-onnx.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01129fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)  # Random action\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # Exploitation\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.model(next_state)).item()\n",
    "            target_f = self.model(state).detach()\n",
    "            target_f[0][action] = target\n",
    "            self.model.zero_grad()\n",
    "            loss = nn.functional.mse_loss(self.model(state), target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2258b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: -6, Epsilon: 1.00\n",
      "Episode 2/1000, Total Reward: -30, Epsilon: 1.00\n",
      "Episode 3/1000, Total Reward: -6, Epsilon: 1.00\n",
      "Episode 4/1000, Total Reward: -41, Epsilon: 0.99\n",
      "Episode 5/1000, Total Reward: -71, Epsilon: 0.99\n",
      "Episode 6/1000, Total Reward: -7, Epsilon: 0.99\n",
      "Episode 7/1000, Total Reward: -12, Epsilon: 0.98\n",
      "Episode 8/1000, Total Reward: -78, Epsilon: 0.98\n",
      "Episode 9/1000, Total Reward: -15, Epsilon: 0.97\n",
      "Episode 10/1000, Total Reward: -19, Epsilon: 0.97\n",
      "Episode 11/1000, Total Reward: -6, Epsilon: 0.96\n",
      "Episode 12/1000, Total Reward: -13, Epsilon: 0.96\n",
      "Episode 13/1000, Total Reward: -7, Epsilon: 0.95\n",
      "Episode 14/1000, Total Reward: -33, Epsilon: 0.95\n",
      "Episode 15/1000, Total Reward: -5, Epsilon: 0.94\n",
      "Episode 16/1000, Total Reward: -11, Epsilon: 0.94\n",
      "Episode 17/1000, Total Reward: -13, Epsilon: 0.93\n",
      "Episode 18/1000, Total Reward: -10, Epsilon: 0.93\n",
      "Episode 19/1000, Total Reward: -21, Epsilon: 0.92\n",
      "Episode 20/1000, Total Reward: -17, Epsilon: 0.92\n",
      "Episode 21/1000, Total Reward: -6, Epsilon: 0.91\n",
      "Episode 22/1000, Total Reward: -71, Epsilon: 0.91\n",
      "Episode 23/1000, Total Reward: -44, Epsilon: 0.90\n",
      "Episode 24/1000, Total Reward: -20, Epsilon: 0.90\n",
      "Episode 25/1000, Total Reward: -5, Epsilon: 0.90\n",
      "Episode 26/1000, Total Reward: -28, Epsilon: 0.89\n",
      "Episode 27/1000, Total Reward: -68, Epsilon: 0.89\n",
      "Episode 28/1000, Total Reward: -21, Epsilon: 0.88\n",
      "Episode 29/1000, Total Reward: -12, Epsilon: 0.88\n",
      "Episode 30/1000, Total Reward: -9, Epsilon: 0.87\n",
      "Episode 31/1000, Total Reward: -17, Epsilon: 0.87\n",
      "Episode 32/1000, Total Reward: -5, Epsilon: 0.86\n",
      "Episode 33/1000, Total Reward: -5, Epsilon: 0.86\n",
      "Episode 34/1000, Total Reward: -59, Epsilon: 0.86\n",
      "Episode 35/1000, Total Reward: -14, Epsilon: 0.85\n",
      "Episode 36/1000, Total Reward: -31, Epsilon: 0.85\n",
      "Episode 37/1000, Total Reward: -38, Epsilon: 0.84\n",
      "Episode 38/1000, Total Reward: 2, Epsilon: 0.84\n",
      "Episode 39/1000, Total Reward: -44, Epsilon: 0.83\n",
      "Episode 40/1000, Total Reward: -22, Epsilon: 0.83\n",
      "Episode 41/1000, Total Reward: -10, Epsilon: 0.83\n",
      "Episode 42/1000, Total Reward: -5, Epsilon: 0.82\n",
      "Episode 43/1000, Total Reward: -18, Epsilon: 0.82\n",
      "Episode 44/1000, Total Reward: -47, Epsilon: 0.81\n",
      "Episode 45/1000, Total Reward: -18, Epsilon: 0.81\n",
      "Episode 46/1000, Total Reward: -40, Epsilon: 0.81\n",
      "Episode 47/1000, Total Reward: -6, Epsilon: 0.80\n",
      "Episode 48/1000, Total Reward: -5, Epsilon: 0.80\n",
      "Episode 49/1000, Total Reward: -7, Epsilon: 0.79\n",
      "Episode 50/1000, Total Reward: -7, Epsilon: 0.79\n",
      "Episode 51/1000, Total Reward: -8, Epsilon: 0.79\n",
      "Episode 52/1000, Total Reward: -14, Epsilon: 0.78\n",
      "Episode 53/1000, Total Reward: -7, Epsilon: 0.78\n",
      "Episode 54/1000, Total Reward: -25, Epsilon: 0.77\n",
      "Episode 55/1000, Total Reward: -13, Epsilon: 0.77\n",
      "Episode 56/1000, Total Reward: -39, Epsilon: 0.77\n",
      "Episode 57/1000, Total Reward: -76, Epsilon: 0.76\n",
      "Episode 58/1000, Total Reward: -37, Epsilon: 0.76\n",
      "Episode 59/1000, Total Reward: -34, Epsilon: 0.76\n",
      "Episode 60/1000, Total Reward: -13, Epsilon: 0.75\n",
      "Episode 61/1000, Total Reward: -14, Epsilon: 0.75\n",
      "Episode 62/1000, Total Reward: -29, Epsilon: 0.74\n",
      "Episode 63/1000, Total Reward: -5, Epsilon: 0.74\n",
      "Episode 64/1000, Total Reward: -7, Epsilon: 0.74\n",
      "Episode 65/1000, Total Reward: -1, Epsilon: 0.73\n",
      "Episode 66/1000, Total Reward: -29, Epsilon: 0.73\n",
      "Episode 67/1000, Total Reward: -23, Epsilon: 0.73\n",
      "Episode 68/1000, Total Reward: -10, Epsilon: 0.72\n",
      "Episode 69/1000, Total Reward: -9, Epsilon: 0.72\n",
      "Episode 70/1000, Total Reward: 6, Epsilon: 0.71\n",
      "Episode 71/1000, Total Reward: -12, Epsilon: 0.71\n",
      "Episode 72/1000, Total Reward: -12, Epsilon: 0.71\n",
      "Episode 73/1000, Total Reward: -46, Epsilon: 0.70\n",
      "Episode 74/1000, Total Reward: -21, Epsilon: 0.70\n",
      "Episode 75/1000, Total Reward: -29, Epsilon: 0.70\n",
      "Episode 76/1000, Total Reward: -31, Epsilon: 0.69\n",
      "Episode 77/1000, Total Reward: -30, Epsilon: 0.69\n",
      "Episode 78/1000, Total Reward: -11, Epsilon: 0.69\n",
      "Episode 79/1000, Total Reward: -9, Epsilon: 0.68\n",
      "Episode 80/1000, Total Reward: -9, Epsilon: 0.68\n",
      "Episode 81/1000, Total Reward: -18, Epsilon: 0.68\n",
      "Episode 82/1000, Total Reward: -14, Epsilon: 0.67\n",
      "Episode 83/1000, Total Reward: -34, Epsilon: 0.67\n",
      "Episode 84/1000, Total Reward: 10, Epsilon: 0.67\n",
      "Episode 85/1000, Total Reward: -12, Epsilon: 0.66\n",
      "Episode 86/1000, Total Reward: 1, Epsilon: 0.66\n",
      "Episode 87/1000, Total Reward: -5, Epsilon: 0.66\n",
      "Episode 88/1000, Total Reward: -36, Epsilon: 0.65\n",
      "Episode 89/1000, Total Reward: -10, Epsilon: 0.65\n",
      "Episode 90/1000, Total Reward: -12, Epsilon: 0.65\n",
      "Episode 91/1000, Total Reward: -8, Epsilon: 0.64\n",
      "Episode 92/1000, Total Reward: -6, Epsilon: 0.64\n",
      "Episode 93/1000, Total Reward: -7, Epsilon: 0.64\n",
      "Episode 94/1000, Total Reward: -18, Epsilon: 0.63\n",
      "Episode 95/1000, Total Reward: -32, Epsilon: 0.63\n",
      "Episode 96/1000, Total Reward: -6, Epsilon: 0.63\n",
      "Episode 97/1000, Total Reward: -15, Epsilon: 0.62\n",
      "Episode 98/1000, Total Reward: -38, Epsilon: 0.62\n",
      "Episode 99/1000, Total Reward: -8, Epsilon: 0.62\n",
      "Episode 100/1000, Total Reward: -27, Epsilon: 0.61\n",
      "Episode 101/1000, Total Reward: 7, Epsilon: 0.61\n",
      "Episode 102/1000, Total Reward: -33, Epsilon: 0.61\n",
      "Episode 103/1000, Total Reward: -8, Epsilon: 0.61\n",
      "Episode 104/1000, Total Reward: 10, Epsilon: 0.60\n",
      "Episode 105/1000, Total Reward: -9, Epsilon: 0.60\n",
      "Episode 106/1000, Total Reward: -14, Epsilon: 0.60\n",
      "Episode 107/1000, Total Reward: -37, Epsilon: 0.59\n",
      "Episode 108/1000, Total Reward: -16, Epsilon: 0.59\n",
      "Episode 109/1000, Total Reward: -19, Epsilon: 0.59\n",
      "Episode 110/1000, Total Reward: 8, Epsilon: 0.58\n",
      "Episode 111/1000, Total Reward: -11, Epsilon: 0.58\n",
      "Episode 112/1000, Total Reward: -5, Epsilon: 0.58\n",
      "Episode 113/1000, Total Reward: 10, Epsilon: 0.58\n",
      "Episode 114/1000, Total Reward: -40, Epsilon: 0.57\n",
      "Episode 115/1000, Total Reward: -9, Epsilon: 0.57\n",
      "Episode 116/1000, Total Reward: -9, Epsilon: 0.57\n",
      "Episode 117/1000, Total Reward: -7, Epsilon: 0.56\n",
      "Episode 118/1000, Total Reward: -25, Epsilon: 0.56\n",
      "Episode 119/1000, Total Reward: -5, Epsilon: 0.56\n",
      "Episode 120/1000, Total Reward: -6, Epsilon: 0.56\n",
      "Episode 121/1000, Total Reward: -18, Epsilon: 0.55\n",
      "Episode 122/1000, Total Reward: -11, Epsilon: 0.55\n",
      "Episode 123/1000, Total Reward: -15, Epsilon: 0.55\n",
      "Episode 124/1000, Total Reward: -18, Epsilon: 0.55\n",
      "Episode 125/1000, Total Reward: -15, Epsilon: 0.54\n",
      "Episode 126/1000, Total Reward: -23, Epsilon: 0.54\n",
      "Episode 127/1000, Total Reward: -5, Epsilon: 0.54\n",
      "Episode 128/1000, Total Reward: -17, Epsilon: 0.53\n",
      "Episode 129/1000, Total Reward: -6, Epsilon: 0.53\n",
      "Episode 130/1000, Total Reward: -5, Epsilon: 0.53\n",
      "Episode 131/1000, Total Reward: -14, Epsilon: 0.53\n",
      "Episode 132/1000, Total Reward: -7, Epsilon: 0.52\n",
      "Episode 133/1000, Total Reward: -6, Epsilon: 0.52\n",
      "Episode 134/1000, Total Reward: -16, Epsilon: 0.52\n",
      "Episode 135/1000, Total Reward: -7, Epsilon: 0.52\n",
      "Episode 136/1000, Total Reward: -6, Epsilon: 0.51\n",
      "Episode 137/1000, Total Reward: -32, Epsilon: 0.51\n",
      "Episode 138/1000, Total Reward: -7, Epsilon: 0.51\n",
      "Episode 139/1000, Total Reward: -5, Epsilon: 0.51\n",
      "Episode 140/1000, Total Reward: -26, Epsilon: 0.50\n",
      "Episode 141/1000, Total Reward: -7, Epsilon: 0.50\n",
      "Episode 142/1000, Total Reward: -24, Epsilon: 0.50\n",
      "Episode 143/1000, Total Reward: -6, Epsilon: 0.50\n",
      "Episode 144/1000, Total Reward: -8, Epsilon: 0.49\n",
      "Episode 145/1000, Total Reward: -10, Epsilon: 0.49\n",
      "Episode 146/1000, Total Reward: -9, Epsilon: 0.49\n",
      "Episode 147/1000, Total Reward: -6, Epsilon: 0.49\n",
      "Episode 148/1000, Total Reward: -12, Epsilon: 0.48\n",
      "Episode 149/1000, Total Reward: -14, Epsilon: 0.48\n",
      "Episode 150/1000, Total Reward: -21, Epsilon: 0.48\n",
      "Episode 151/1000, Total Reward: -17, Epsilon: 0.48\n",
      "Episode 152/1000, Total Reward: -15, Epsilon: 0.47\n",
      "Episode 153/1000, Total Reward: -13, Epsilon: 0.47\n",
      "Episode 154/1000, Total Reward: -18, Epsilon: 0.47\n",
      "Episode 155/1000, Total Reward: -5, Epsilon: 0.47\n",
      "Episode 156/1000, Total Reward: -19, Epsilon: 0.46\n",
      "Episode 157/1000, Total Reward: -28, Epsilon: 0.46\n",
      "Episode 158/1000, Total Reward: -6, Epsilon: 0.46\n",
      "Episode 159/1000, Total Reward: -5, Epsilon: 0.46\n",
      "Episode 160/1000, Total Reward: -21, Epsilon: 0.46\n",
      "Episode 161/1000, Total Reward: -9, Epsilon: 0.45\n",
      "Episode 162/1000, Total Reward: -16, Epsilon: 0.45\n",
      "Episode 163/1000, Total Reward: -20, Epsilon: 0.45\n",
      "Episode 164/1000, Total Reward: -9, Epsilon: 0.45\n",
      "Episode 165/1000, Total Reward: -14, Epsilon: 0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 166/1000, Total Reward: -7, Epsilon: 0.44\n",
      "Episode 167/1000, Total Reward: -18, Epsilon: 0.44\n",
      "Episode 168/1000, Total Reward: -5, Epsilon: 0.44\n",
      "Episode 169/1000, Total Reward: -5, Epsilon: 0.44\n",
      "Episode 170/1000, Total Reward: -18, Epsilon: 0.43\n",
      "Episode 171/1000, Total Reward: -10, Epsilon: 0.43\n",
      "Episode 172/1000, Total Reward: -6, Epsilon: 0.43\n",
      "Episode 173/1000, Total Reward: -9, Epsilon: 0.43\n",
      "Episode 174/1000, Total Reward: -9, Epsilon: 0.42\n",
      "Episode 175/1000, Total Reward: -6, Epsilon: 0.42\n",
      "Episode 176/1000, Total Reward: -16, Epsilon: 0.42\n",
      "Episode 177/1000, Total Reward: -6, Epsilon: 0.42\n",
      "Episode 178/1000, Total Reward: -13, Epsilon: 0.42\n",
      "Episode 179/1000, Total Reward: -6, Epsilon: 0.41\n",
      "Episode 180/1000, Total Reward: -19, Epsilon: 0.41\n",
      "Episode 181/1000, Total Reward: -11, Epsilon: 0.41\n",
      "Episode 182/1000, Total Reward: 6, Epsilon: 0.41\n",
      "Episode 183/1000, Total Reward: -12, Epsilon: 0.41\n",
      "Episode 184/1000, Total Reward: -15, Epsilon: 0.40\n",
      "Episode 185/1000, Total Reward: -20, Epsilon: 0.40\n",
      "Episode 186/1000, Total Reward: -6, Epsilon: 0.40\n",
      "Episode 187/1000, Total Reward: -11, Epsilon: 0.40\n",
      "Episode 188/1000, Total Reward: -13, Epsilon: 0.40\n",
      "Episode 189/1000, Total Reward: -12, Epsilon: 0.39\n",
      "Episode 190/1000, Total Reward: -14, Epsilon: 0.39\n",
      "Episode 191/1000, Total Reward: -12, Epsilon: 0.39\n",
      "Episode 192/1000, Total Reward: -10, Epsilon: 0.39\n",
      "Episode 193/1000, Total Reward: -20, Epsilon: 0.39\n",
      "Episode 194/1000, Total Reward: -19, Epsilon: 0.38\n",
      "Episode 195/1000, Total Reward: -10, Epsilon: 0.38\n",
      "Episode 196/1000, Total Reward: -12, Epsilon: 0.38\n",
      "Episode 197/1000, Total Reward: -18, Epsilon: 0.38\n",
      "Episode 198/1000, Total Reward: -14, Epsilon: 0.38\n",
      "Episode 199/1000, Total Reward: -5, Epsilon: 0.37\n",
      "Episode 200/1000, Total Reward: -11, Epsilon: 0.37\n",
      "Episode 201/1000, Total Reward: -7, Epsilon: 0.37\n",
      "Episode 202/1000, Total Reward: -9, Epsilon: 0.37\n",
      "Episode 203/1000, Total Reward: -6, Epsilon: 0.37\n",
      "Episode 204/1000, Total Reward: -20, Epsilon: 0.37\n",
      "Episode 205/1000, Total Reward: -5, Epsilon: 0.36\n",
      "Episode 206/1000, Total Reward: -5, Epsilon: 0.36\n",
      "Episode 207/1000, Total Reward: -9, Epsilon: 0.36\n",
      "Episode 208/1000, Total Reward: -5, Epsilon: 0.36\n",
      "Episode 209/1000, Total Reward: -8, Epsilon: 0.36\n",
      "Episode 210/1000, Total Reward: -26, Epsilon: 0.35\n",
      "Episode 211/1000, Total Reward: -9, Epsilon: 0.35\n",
      "Episode 212/1000, Total Reward: -14, Epsilon: 0.35\n",
      "Episode 213/1000, Total Reward: -11, Epsilon: 0.35\n",
      "Episode 214/1000, Total Reward: -19, Epsilon: 0.35\n",
      "Episode 215/1000, Total Reward: -26, Epsilon: 0.35\n",
      "Episode 216/1000, Total Reward: -17, Epsilon: 0.34\n",
      "Episode 217/1000, Total Reward: -13, Epsilon: 0.34\n",
      "Episode 218/1000, Total Reward: -11, Epsilon: 0.34\n",
      "Episode 219/1000, Total Reward: -19, Epsilon: 0.34\n",
      "Episode 220/1000, Total Reward: -7, Epsilon: 0.34\n",
      "Episode 221/1000, Total Reward: -13, Epsilon: 0.34\n",
      "Episode 222/1000, Total Reward: -8, Epsilon: 0.33\n",
      "Episode 223/1000, Total Reward: -8, Epsilon: 0.33\n",
      "Episode 224/1000, Total Reward: -10, Epsilon: 0.33\n",
      "Episode 225/1000, Total Reward: -6, Epsilon: 0.33\n",
      "Episode 226/1000, Total Reward: -18, Epsilon: 0.33\n",
      "Episode 227/1000, Total Reward: -5, Epsilon: 0.33\n",
      "Episode 228/1000, Total Reward: -16, Epsilon: 0.32\n",
      "Episode 229/1000, Total Reward: -6, Epsilon: 0.32\n",
      "Episode 230/1000, Total Reward: -5, Epsilon: 0.32\n",
      "Episode 231/1000, Total Reward: -12, Epsilon: 0.32\n",
      "Episode 232/1000, Total Reward: 4, Epsilon: 0.32\n",
      "Episode 233/1000, Total Reward: -19, Epsilon: 0.32\n",
      "Episode 234/1000, Total Reward: -8, Epsilon: 0.31\n",
      "Episode 235/1000, Total Reward: -5, Epsilon: 0.31\n",
      "Episode 236/1000, Total Reward: -12, Epsilon: 0.31\n",
      "Episode 237/1000, Total Reward: -17, Epsilon: 0.31\n",
      "Episode 238/1000, Total Reward: -5, Epsilon: 0.31\n",
      "Episode 239/1000, Total Reward: -17, Epsilon: 0.31\n",
      "Episode 240/1000, Total Reward: -5, Epsilon: 0.30\n",
      "Episode 241/1000, Total Reward: -9, Epsilon: 0.30\n",
      "Episode 242/1000, Total Reward: -6, Epsilon: 0.30\n",
      "Episode 243/1000, Total Reward: -6, Epsilon: 0.30\n",
      "Episode 244/1000, Total Reward: 10, Epsilon: 0.30\n",
      "Episode 245/1000, Total Reward: 9, Epsilon: 0.30\n",
      "Episode 246/1000, Total Reward: -9, Epsilon: 0.30\n",
      "Episode 247/1000, Total Reward: -7, Epsilon: 0.29\n",
      "Episode 248/1000, Total Reward: -13, Epsilon: 0.29\n",
      "Episode 249/1000, Total Reward: -8, Epsilon: 0.29\n",
      "Episode 250/1000, Total Reward: -22, Epsilon: 0.29\n",
      "Episode 251/1000, Total Reward: -9, Epsilon: 0.29\n",
      "Episode 252/1000, Total Reward: -7, Epsilon: 0.29\n",
      "Episode 253/1000, Total Reward: -10, Epsilon: 0.29\n",
      "Episode 254/1000, Total Reward: -9, Epsilon: 0.28\n",
      "Episode 255/1000, Total Reward: -5, Epsilon: 0.28\n",
      "Episode 256/1000, Total Reward: -9, Epsilon: 0.28\n",
      "Episode 257/1000, Total Reward: -14, Epsilon: 0.28\n",
      "Episode 258/1000, Total Reward: -15, Epsilon: 0.28\n",
      "Episode 259/1000, Total Reward: -8, Epsilon: 0.28\n",
      "Episode 260/1000, Total Reward: -10, Epsilon: 0.28\n",
      "Episode 261/1000, Total Reward: -7, Epsilon: 0.27\n",
      "Episode 262/1000, Total Reward: 10, Epsilon: 0.27\n",
      "Episode 263/1000, Total Reward: -5, Epsilon: 0.27\n",
      "Episode 264/1000, Total Reward: -7, Epsilon: 0.27\n",
      "Episode 265/1000, Total Reward: -6, Epsilon: 0.27\n",
      "Episode 266/1000, Total Reward: -14, Epsilon: 0.27\n",
      "Episode 267/1000, Total Reward: -11, Epsilon: 0.27\n",
      "Episode 268/1000, Total Reward: -7, Epsilon: 0.26\n",
      "Episode 269/1000, Total Reward: -15, Epsilon: 0.26\n",
      "Episode 270/1000, Total Reward: -12, Epsilon: 0.26\n",
      "Episode 271/1000, Total Reward: -11, Epsilon: 0.26\n",
      "Episode 272/1000, Total Reward: -11, Epsilon: 0.26\n",
      "Episode 273/1000, Total Reward: -9, Epsilon: 0.26\n",
      "Episode 274/1000, Total Reward: -6, Epsilon: 0.26\n",
      "Episode 275/1000, Total Reward: 2, Epsilon: 0.26\n",
      "Episode 276/1000, Total Reward: -8, Epsilon: 0.25\n",
      "Episode 277/1000, Total Reward: -11, Epsilon: 0.25\n",
      "Episode 278/1000, Total Reward: -9, Epsilon: 0.25\n",
      "Episode 279/1000, Total Reward: -10, Epsilon: 0.25\n",
      "Episode 280/1000, Total Reward: 2, Epsilon: 0.25\n",
      "Episode 281/1000, Total Reward: 3, Epsilon: 0.25\n",
      "Episode 282/1000, Total Reward: -12, Epsilon: 0.25\n",
      "Episode 283/1000, Total Reward: -5, Epsilon: 0.25\n",
      "Episode 284/1000, Total Reward: -9, Epsilon: 0.24\n",
      "Episode 285/1000, Total Reward: -8, Epsilon: 0.24\n",
      "Episode 286/1000, Total Reward: -9, Epsilon: 0.24\n",
      "Episode 287/1000, Total Reward: -11, Epsilon: 0.24\n",
      "Episode 288/1000, Total Reward: -11, Epsilon: 0.24\n",
      "Episode 289/1000, Total Reward: -9, Epsilon: 0.24\n",
      "Episode 290/1000, Total Reward: -7, Epsilon: 0.24\n",
      "Episode 291/1000, Total Reward: -10, Epsilon: 0.24\n",
      "Episode 292/1000, Total Reward: -17, Epsilon: 0.23\n",
      "Episode 293/1000, Total Reward: -11, Epsilon: 0.23\n",
      "Episode 294/1000, Total Reward: -17, Epsilon: 0.23\n",
      "Episode 295/1000, Total Reward: -17, Epsilon: 0.23\n",
      "Episode 296/1000, Total Reward: -11, Epsilon: 0.23\n",
      "Episode 297/1000, Total Reward: -13, Epsilon: 0.23\n",
      "Episode 298/1000, Total Reward: -10, Epsilon: 0.23\n",
      "Episode 299/1000, Total Reward: -5, Epsilon: 0.23\n",
      "Episode 300/1000, Total Reward: -5, Epsilon: 0.23\n",
      "Episode 301/1000, Total Reward: -8, Epsilon: 0.22\n",
      "Episode 302/1000, Total Reward: 3, Epsilon: 0.22\n",
      "Episode 303/1000, Total Reward: -10, Epsilon: 0.22\n",
      "Episode 304/1000, Total Reward: -12, Epsilon: 0.22\n",
      "Episode 305/1000, Total Reward: -12, Epsilon: 0.22\n",
      "Episode 306/1000, Total Reward: -12, Epsilon: 0.22\n",
      "Episode 307/1000, Total Reward: -7, Epsilon: 0.22\n",
      "Episode 308/1000, Total Reward: 2, Epsilon: 0.22\n",
      "Episode 309/1000, Total Reward: -18, Epsilon: 0.22\n",
      "Episode 310/1000, Total Reward: -21, Epsilon: 0.21\n",
      "Episode 311/1000, Total Reward: 9, Epsilon: 0.21\n",
      "Episode 312/1000, Total Reward: -6, Epsilon: 0.21\n",
      "Episode 313/1000, Total Reward: -13, Epsilon: 0.21\n",
      "Episode 314/1000, Total Reward: -12, Epsilon: 0.21\n",
      "Episode 315/1000, Total Reward: -8, Epsilon: 0.21\n",
      "Episode 316/1000, Total Reward: -12, Epsilon: 0.21\n",
      "Episode 317/1000, Total Reward: -7, Epsilon: 0.21\n",
      "Episode 318/1000, Total Reward: -6, Epsilon: 0.21\n",
      "Episode 319/1000, Total Reward: -6, Epsilon: 0.21\n",
      "Episode 320/1000, Total Reward: -7, Epsilon: 0.20\n",
      "Episode 321/1000, Total Reward: -15, Epsilon: 0.20\n",
      "Episode 322/1000, Total Reward: -11, Epsilon: 0.20\n",
      "Episode 323/1000, Total Reward: -18, Epsilon: 0.20\n",
      "Episode 324/1000, Total Reward: -16, Epsilon: 0.20\n",
      "Episode 325/1000, Total Reward: -9, Epsilon: 0.20\n",
      "Episode 326/1000, Total Reward: -7, Epsilon: 0.20\n",
      "Episode 327/1000, Total Reward: -6, Epsilon: 0.20\n",
      "Episode 328/1000, Total Reward: -14, Epsilon: 0.20\n",
      "Episode 329/1000, Total Reward: -7, Epsilon: 0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 330/1000, Total Reward: -10, Epsilon: 0.19\n",
      "Episode 331/1000, Total Reward: -6, Epsilon: 0.19\n",
      "Episode 332/1000, Total Reward: -10, Epsilon: 0.19\n",
      "Episode 333/1000, Total Reward: -15, Epsilon: 0.19\n",
      "Episode 334/1000, Total Reward: -12, Epsilon: 0.19\n",
      "Episode 335/1000, Total Reward: -6, Epsilon: 0.19\n",
      "Episode 336/1000, Total Reward: -8, Epsilon: 0.19\n",
      "Episode 337/1000, Total Reward: -5, Epsilon: 0.19\n",
      "Episode 338/1000, Total Reward: -8, Epsilon: 0.19\n",
      "Episode 339/1000, Total Reward: -7, Epsilon: 0.19\n",
      "Episode 340/1000, Total Reward: 5, Epsilon: 0.18\n",
      "Episode 341/1000, Total Reward: -17, Epsilon: 0.18\n",
      "Episode 342/1000, Total Reward: -6, Epsilon: 0.18\n",
      "Episode 343/1000, Total Reward: 8, Epsilon: 0.18\n",
      "Episode 344/1000, Total Reward: -6, Epsilon: 0.18\n",
      "Episode 345/1000, Total Reward: -11, Epsilon: 0.18\n",
      "Episode 346/1000, Total Reward: -9, Epsilon: 0.18\n",
      "Episode 347/1000, Total Reward: -12, Epsilon: 0.18\n",
      "Episode 348/1000, Total Reward: -9, Epsilon: 0.18\n",
      "Episode 349/1000, Total Reward: -7, Epsilon: 0.18\n",
      "Episode 350/1000, Total Reward: -7, Epsilon: 0.18\n",
      "Episode 351/1000, Total Reward: -18, Epsilon: 0.17\n",
      "Episode 352/1000, Total Reward: -10, Epsilon: 0.17\n",
      "Episode 353/1000, Total Reward: -6, Epsilon: 0.17\n",
      "Episode 354/1000, Total Reward: 10, Epsilon: 0.17\n",
      "Episode 355/1000, Total Reward: -20, Epsilon: 0.17\n",
      "Episode 356/1000, Total Reward: -12, Epsilon: 0.17\n",
      "Episode 357/1000, Total Reward: -6, Epsilon: 0.17\n",
      "Episode 358/1000, Total Reward: -15, Epsilon: 0.17\n",
      "Episode 359/1000, Total Reward: -10, Epsilon: 0.17\n",
      "Episode 360/1000, Total Reward: -10, Epsilon: 0.17\n",
      "Episode 361/1000, Total Reward: -6, Epsilon: 0.17\n",
      "Episode 362/1000, Total Reward: -15, Epsilon: 0.17\n",
      "Episode 363/1000, Total Reward: -7, Epsilon: 0.16\n",
      "Episode 364/1000, Total Reward: -12, Epsilon: 0.16\n",
      "Episode 365/1000, Total Reward: -15, Epsilon: 0.16\n",
      "Episode 366/1000, Total Reward: -8, Epsilon: 0.16\n",
      "Episode 367/1000, Total Reward: -12, Epsilon: 0.16\n",
      "Episode 368/1000, Total Reward: -8, Epsilon: 0.16\n",
      "Episode 369/1000, Total Reward: -7, Epsilon: 0.16\n",
      "Episode 370/1000, Total Reward: -9, Epsilon: 0.16\n",
      "Episode 371/1000, Total Reward: -10, Epsilon: 0.16\n",
      "Episode 372/1000, Total Reward: -14, Epsilon: 0.16\n",
      "Episode 373/1000, Total Reward: -12, Epsilon: 0.16\n",
      "Episode 374/1000, Total Reward: -12, Epsilon: 0.16\n",
      "Episode 375/1000, Total Reward: -9, Epsilon: 0.15\n",
      "Episode 376/1000, Total Reward: -7, Epsilon: 0.15\n",
      "Episode 377/1000, Total Reward: -17, Epsilon: 0.15\n",
      "Episode 378/1000, Total Reward: -14, Epsilon: 0.15\n",
      "Episode 379/1000, Total Reward: -18, Epsilon: 0.15\n",
      "Episode 380/1000, Total Reward: -8, Epsilon: 0.15\n",
      "Episode 381/1000, Total Reward: -5, Epsilon: 0.15\n",
      "Episode 382/1000, Total Reward: 7, Epsilon: 0.15\n",
      "Episode 383/1000, Total Reward: -20, Epsilon: 0.15\n",
      "Episode 384/1000, Total Reward: -11, Epsilon: 0.15\n",
      "Episode 385/1000, Total Reward: -13, Epsilon: 0.15\n",
      "Episode 386/1000, Total Reward: -14, Epsilon: 0.15\n",
      "Episode 387/1000, Total Reward: -13, Epsilon: 0.15\n",
      "Episode 388/1000, Total Reward: -12, Epsilon: 0.15\n",
      "Episode 389/1000, Total Reward: -10, Epsilon: 0.14\n",
      "Episode 390/1000, Total Reward: -9, Epsilon: 0.14\n",
      "Episode 391/1000, Total Reward: -8, Epsilon: 0.14\n",
      "Episode 392/1000, Total Reward: -10, Epsilon: 0.14\n",
      "Episode 393/1000, Total Reward: -7, Epsilon: 0.14\n",
      "Episode 394/1000, Total Reward: -16, Epsilon: 0.14\n",
      "Episode 395/1000, Total Reward: -15, Epsilon: 0.14\n",
      "Episode 396/1000, Total Reward: -10, Epsilon: 0.14\n",
      "Episode 397/1000, Total Reward: -10, Epsilon: 0.14\n",
      "Episode 398/1000, Total Reward: -11, Epsilon: 0.14\n",
      "Episode 399/1000, Total Reward: -11, Epsilon: 0.14\n",
      "Episode 400/1000, Total Reward: -10, Epsilon: 0.14\n",
      "Episode 401/1000, Total Reward: -6, Epsilon: 0.14\n",
      "Episode 402/1000, Total Reward: -6, Epsilon: 0.14\n",
      "Episode 403/1000, Total Reward: 10, Epsilon: 0.13\n",
      "Episode 404/1000, Total Reward: 6, Epsilon: 0.13\n",
      "Episode 405/1000, Total Reward: -6, Epsilon: 0.13\n",
      "Episode 406/1000, Total Reward: -10, Epsilon: 0.13\n",
      "Episode 407/1000, Total Reward: -14, Epsilon: 0.13\n",
      "Episode 408/1000, Total Reward: 3, Epsilon: 0.13\n",
      "Episode 409/1000, Total Reward: -7, Epsilon: 0.13\n",
      "Episode 410/1000, Total Reward: -7, Epsilon: 0.13\n",
      "Episode 411/1000, Total Reward: -16, Epsilon: 0.13\n",
      "Episode 412/1000, Total Reward: -7, Epsilon: 0.13\n",
      "Episode 413/1000, Total Reward: -14, Epsilon: 0.13\n",
      "Episode 414/1000, Total Reward: -14, Epsilon: 0.13\n",
      "Episode 415/1000, Total Reward: -15, Epsilon: 0.13\n",
      "Episode 416/1000, Total Reward: -11, Epsilon: 0.13\n",
      "Episode 417/1000, Total Reward: -12, Epsilon: 0.13\n",
      "Episode 418/1000, Total Reward: -9, Epsilon: 0.12\n",
      "Episode 419/1000, Total Reward: -7, Epsilon: 0.12\n",
      "Episode 420/1000, Total Reward: 8, Epsilon: 0.12\n",
      "Episode 421/1000, Total Reward: -15, Epsilon: 0.12\n",
      "Episode 422/1000, Total Reward: -6, Epsilon: 0.12\n",
      "Episode 423/1000, Total Reward: -9, Epsilon: 0.12\n",
      "Episode 424/1000, Total Reward: -14, Epsilon: 0.12\n",
      "Episode 425/1000, Total Reward: -8, Epsilon: 0.12\n",
      "Episode 426/1000, Total Reward: -9, Epsilon: 0.12\n",
      "Episode 427/1000, Total Reward: -8, Epsilon: 0.12\n",
      "Episode 428/1000, Total Reward: -12, Epsilon: 0.12\n",
      "Episode 429/1000, Total Reward: -412, Epsilon: 0.12\n",
      "Episode 430/1000, Total Reward: 7, Epsilon: 0.12\n",
      "Episode 431/1000, Total Reward: -14, Epsilon: 0.12\n",
      "Episode 432/1000, Total Reward: 3, Epsilon: 0.12\n",
      "Episode 433/1000, Total Reward: -13, Epsilon: 0.12\n",
      "Episode 434/1000, Total Reward: 6, Epsilon: 0.12\n",
      "Episode 435/1000, Total Reward: -6, Epsilon: 0.11\n",
      "Episode 436/1000, Total Reward: -12, Epsilon: 0.11\n",
      "Episode 437/1000, Total Reward: -6, Epsilon: 0.11\n",
      "Episode 438/1000, Total Reward: 9, Epsilon: 0.11\n",
      "Episode 439/1000, Total Reward: 10, Epsilon: 0.11\n",
      "Episode 440/1000, Total Reward: -8, Epsilon: 0.11\n",
      "Episode 441/1000, Total Reward: -13, Epsilon: 0.11\n",
      "Episode 442/1000, Total Reward: -11, Epsilon: 0.11\n",
      "Episode 443/1000, Total Reward: -10, Epsilon: 0.11\n",
      "Episode 444/1000, Total Reward: -9, Epsilon: 0.11\n",
      "Episode 445/1000, Total Reward: -13, Epsilon: 0.11\n",
      "Episode 446/1000, Total Reward: 4, Epsilon: 0.11\n",
      "Episode 447/1000, Total Reward: -13, Epsilon: 0.11\n",
      "Episode 448/1000, Total Reward: -11, Epsilon: 0.11\n",
      "Episode 449/1000, Total Reward: -9, Epsilon: 0.11\n",
      "Episode 450/1000, Total Reward: 6, Epsilon: 0.11\n",
      "Episode 451/1000, Total Reward: -7, Epsilon: 0.11\n",
      "Episode 452/1000, Total Reward: -5, Epsilon: 0.11\n",
      "Episode 453/1000, Total Reward: -8, Epsilon: 0.10\n",
      "Episode 454/1000, Total Reward: -5, Epsilon: 0.10\n",
      "Episode 455/1000, Total Reward: -5, Epsilon: 0.10\n",
      "Episode 456/1000, Total Reward: 7, Epsilon: 0.10\n",
      "Episode 457/1000, Total Reward: -10, Epsilon: 0.10\n",
      "Episode 458/1000, Total Reward: -9, Epsilon: 0.10\n",
      "Episode 459/1000, Total Reward: -6, Epsilon: 0.10\n",
      "Episode 460/1000, Total Reward: -6, Epsilon: 0.10\n",
      "Episode 461/1000, Total Reward: -14, Epsilon: 0.10\n",
      "Episode 462/1000, Total Reward: -6, Epsilon: 0.10\n",
      "Episode 463/1000, Total Reward: -11, Epsilon: 0.10\n",
      "Episode 464/1000, Total Reward: -10, Epsilon: 0.10\n",
      "Episode 465/1000, Total Reward: -12, Epsilon: 0.10\n",
      "Episode 466/1000, Total Reward: -14, Epsilon: 0.10\n",
      "Episode 467/1000, Total Reward: 9, Epsilon: 0.10\n",
      "Episode 468/1000, Total Reward: -8, Epsilon: 0.10\n",
      "Episode 469/1000, Total Reward: 10, Epsilon: 0.10\n",
      "Episode 470/1000, Total Reward: -11, Epsilon: 0.10\n",
      "Episode 471/1000, Total Reward: -6, Epsilon: 0.10\n",
      "Episode 472/1000, Total Reward: -7, Epsilon: 0.10\n",
      "Episode 473/1000, Total Reward: -8, Epsilon: 0.09\n",
      "Episode 474/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 475/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 476/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 477/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 478/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 479/1000, Total Reward: -8, Epsilon: 0.09\n",
      "Episode 480/1000, Total Reward: -6, Epsilon: 0.09\n",
      "Episode 481/1000, Total Reward: -6, Epsilon: 0.09\n",
      "Episode 482/1000, Total Reward: -13, Epsilon: 0.09\n",
      "Episode 483/1000, Total Reward: 10, Epsilon: 0.09\n",
      "Episode 484/1000, Total Reward: -12, Epsilon: 0.09\n",
      "Episode 485/1000, Total Reward: 8, Epsilon: 0.09\n",
      "Episode 486/1000, Total Reward: -9, Epsilon: 0.09\n",
      "Episode 487/1000, Total Reward: -9, Epsilon: 0.09\n",
      "Episode 488/1000, Total Reward: 9, Epsilon: 0.09\n",
      "Episode 489/1000, Total Reward: -6, Epsilon: 0.09\n",
      "Episode 490/1000, Total Reward: 7, Epsilon: 0.09\n",
      "Episode 491/1000, Total Reward: -14, Epsilon: 0.09\n",
      "Episode 492/1000, Total Reward: -9, Epsilon: 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 493/1000, Total Reward: -10, Epsilon: 0.09\n",
      "Episode 494/1000, Total Reward: -6, Epsilon: 0.09\n",
      "Episode 495/1000, Total Reward: -6, Epsilon: 0.08\n",
      "Episode 496/1000, Total Reward: -6, Epsilon: 0.08\n",
      "Episode 497/1000, Total Reward: 8, Epsilon: 0.08\n",
      "Episode 498/1000, Total Reward: -6, Epsilon: 0.08\n",
      "Episode 499/1000, Total Reward: -14, Epsilon: 0.08\n",
      "Episode 500/1000, Total Reward: -11, Epsilon: 0.08\n",
      "Episode 501/1000, Total Reward: -11, Epsilon: 0.08\n",
      "Episode 502/1000, Total Reward: -13, Epsilon: 0.08\n",
      "Episode 503/1000, Total Reward: -14, Epsilon: 0.08\n",
      "Episode 504/1000, Total Reward: -13, Epsilon: 0.08\n",
      "Episode 505/1000, Total Reward: -17, Epsilon: 0.08\n",
      "Episode 506/1000, Total Reward: -12, Epsilon: 0.08\n",
      "Episode 507/1000, Total Reward: -7, Epsilon: 0.08\n",
      "Episode 508/1000, Total Reward: -16, Epsilon: 0.08\n",
      "Episode 509/1000, Total Reward: 4, Epsilon: 0.08\n",
      "Episode 510/1000, Total Reward: -15, Epsilon: 0.08\n",
      "Episode 511/1000, Total Reward: 10, Epsilon: 0.08\n",
      "Episode 512/1000, Total Reward: -11, Epsilon: 0.08\n",
      "Episode 513/1000, Total Reward: -12, Epsilon: 0.08\n",
      "Episode 514/1000, Total Reward: -7, Epsilon: 0.08\n",
      "Episode 515/1000, Total Reward: -11, Epsilon: 0.08\n",
      "Episode 516/1000, Total Reward: -12, Epsilon: 0.08\n",
      "Episode 517/1000, Total Reward: -12, Epsilon: 0.08\n",
      "Episode 518/1000, Total Reward: 8, Epsilon: 0.08\n",
      "Episode 519/1000, Total Reward: -8, Epsilon: 0.08\n",
      "Episode 520/1000, Total Reward: -13, Epsilon: 0.07\n",
      "Episode 521/1000, Total Reward: -14, Epsilon: 0.07\n",
      "Episode 522/1000, Total Reward: -5, Epsilon: 0.07\n",
      "Episode 523/1000, Total Reward: -14, Epsilon: 0.07\n",
      "Episode 524/1000, Total Reward: -12, Epsilon: 0.07\n",
      "Episode 525/1000, Total Reward: -13, Epsilon: 0.07\n",
      "Episode 526/1000, Total Reward: -5, Epsilon: 0.07\n",
      "Episode 527/1000, Total Reward: -5, Epsilon: 0.07\n",
      "Episode 528/1000, Total Reward: -7, Epsilon: 0.07\n",
      "Episode 529/1000, Total Reward: -9, Epsilon: 0.07\n",
      "Episode 530/1000, Total Reward: -6, Epsilon: 0.07\n",
      "Episode 531/1000, Total Reward: -7, Epsilon: 0.07\n",
      "Episode 532/1000, Total Reward: -9, Epsilon: 0.07\n",
      "Episode 533/1000, Total Reward: -14, Epsilon: 0.07\n",
      "Episode 534/1000, Total Reward: -6, Epsilon: 0.07\n",
      "Episode 535/1000, Total Reward: -12, Epsilon: 0.07\n",
      "Episode 536/1000, Total Reward: -15, Epsilon: 0.07\n",
      "Episode 537/1000, Total Reward: -9, Epsilon: 0.07\n",
      "Episode 538/1000, Total Reward: -9, Epsilon: 0.07\n",
      "Episode 539/1000, Total Reward: -10, Epsilon: 0.07\n",
      "Episode 540/1000, Total Reward: -7, Epsilon: 0.07\n",
      "Episode 541/1000, Total Reward: -7, Epsilon: 0.07\n",
      "Episode 542/1000, Total Reward: -12, Epsilon: 0.07\n",
      "Episode 543/1000, Total Reward: -11, Epsilon: 0.07\n",
      "Episode 544/1000, Total Reward: -9, Epsilon: 0.07\n",
      "Episode 545/1000, Total Reward: 5, Epsilon: 0.07\n",
      "Episode 546/1000, Total Reward: -10, Epsilon: 0.07\n",
      "Episode 547/1000, Total Reward: -7, Epsilon: 0.07\n",
      "Episode 548/1000, Total Reward: -17, Epsilon: 0.07\n",
      "Episode 549/1000, Total Reward: -7, Epsilon: 0.06\n",
      "Episode 550/1000, Total Reward: -11, Epsilon: 0.06\n",
      "Episode 551/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 552/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 553/1000, Total Reward: -8, Epsilon: 0.06\n",
      "Episode 554/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 555/1000, Total Reward: -11, Epsilon: 0.06\n",
      "Episode 556/1000, Total Reward: -15, Epsilon: 0.06\n",
      "Episode 557/1000, Total Reward: -8, Epsilon: 0.06\n",
      "Episode 558/1000, Total Reward: -15, Epsilon: 0.06\n",
      "Episode 559/1000, Total Reward: -14, Epsilon: 0.06\n",
      "Episode 560/1000, Total Reward: -13, Epsilon: 0.06\n",
      "Episode 561/1000, Total Reward: 5, Epsilon: 0.06\n",
      "Episode 562/1000, Total Reward: -13, Epsilon: 0.06\n",
      "Episode 563/1000, Total Reward: 8, Epsilon: 0.06\n",
      "Episode 564/1000, Total Reward: -9, Epsilon: 0.06\n",
      "Episode 565/1000, Total Reward: -14, Epsilon: 0.06\n",
      "Episode 566/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 567/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 568/1000, Total Reward: -11, Epsilon: 0.06\n",
      "Episode 569/1000, Total Reward: -9, Epsilon: 0.06\n",
      "Episode 570/1000, Total Reward: -12, Epsilon: 0.06\n",
      "Episode 571/1000, Total Reward: 5, Epsilon: 0.06\n",
      "Episode 572/1000, Total Reward: 6, Epsilon: 0.06\n",
      "Episode 573/1000, Total Reward: -10, Epsilon: 0.06\n",
      "Episode 574/1000, Total Reward: -7, Epsilon: 0.06\n",
      "Episode 575/1000, Total Reward: -7, Epsilon: 0.06\n",
      "Episode 576/1000, Total Reward: -11, Epsilon: 0.06\n",
      "Episode 577/1000, Total Reward: -9, Epsilon: 0.06\n",
      "Episode 578/1000, Total Reward: -14, Epsilon: 0.06\n",
      "Episode 579/1000, Total Reward: -11, Epsilon: 0.06\n",
      "Episode 580/1000, Total Reward: -8, Epsilon: 0.06\n",
      "Episode 581/1000, Total Reward: 5, Epsilon: 0.06\n",
      "Episode 582/1000, Total Reward: 7, Epsilon: 0.05\n",
      "Episode 583/1000, Total Reward: -7, Epsilon: 0.05\n",
      "Episode 584/1000, Total Reward: -6, Epsilon: 0.05\n",
      "Episode 585/1000, Total Reward: -7, Epsilon: 0.05\n",
      "Episode 586/1000, Total Reward: -14, Epsilon: 0.05\n",
      "Episode 587/1000, Total Reward: -13, Epsilon: 0.05\n",
      "Episode 588/1000, Total Reward: -7, Epsilon: 0.05\n",
      "Episode 589/1000, Total Reward: -6, Epsilon: 0.05\n",
      "Episode 590/1000, Total Reward: 6, Epsilon: 0.05\n",
      "Episode 591/1000, Total Reward: -13, Epsilon: 0.05\n",
      "Episode 592/1000, Total Reward: 10, Epsilon: 0.05\n",
      "Episode 593/1000, Total Reward: -8, Epsilon: 0.05\n",
      "Episode 594/1000, Total Reward: -5, Epsilon: 0.05\n",
      "Episode 595/1000, Total Reward: -14, Epsilon: 0.05\n",
      "Episode 596/1000, Total Reward: -13, Epsilon: 0.05\n",
      "Episode 597/1000, Total Reward: -8, Epsilon: 0.05\n",
      "Episode 598/1000, Total Reward: -12, Epsilon: 0.05\n",
      "Episode 599/1000, Total Reward: -10, Epsilon: 0.05\n",
      "Episode 600/1000, Total Reward: -10, Epsilon: 0.05\n",
      "Episode 601/1000, Total Reward: -11, Epsilon: 0.05\n",
      "Episode 602/1000, Total Reward: -5, Epsilon: 0.05\n",
      "Episode 603/1000, Total Reward: -10, Epsilon: 0.05\n",
      "Episode 604/1000, Total Reward: -11, Epsilon: 0.05\n",
      "Episode 605/1000, Total Reward: -10, Epsilon: 0.05\n",
      "Episode 606/1000, Total Reward: -9, Epsilon: 0.05\n",
      "Episode 607/1000, Total Reward: -9, Epsilon: 0.05\n",
      "Episode 608/1000, Total Reward: 5, Epsilon: 0.05\n",
      "Episode 609/1000, Total Reward: -9, Epsilon: 0.05\n",
      "Episode 610/1000, Total Reward: 7, Epsilon: 0.05\n",
      "Episode 611/1000, Total Reward: -12, Epsilon: 0.05\n",
      "Episode 612/1000, Total Reward: -11, Epsilon: 0.05\n",
      "Episode 613/1000, Total Reward: -9, Epsilon: 0.05\n",
      "Episode 614/1000, Total Reward: -11, Epsilon: 0.05\n",
      "Episode 615/1000, Total Reward: -13, Epsilon: 0.05\n",
      "Episode 616/1000, Total Reward: -12, Epsilon: 0.05\n",
      "Episode 617/1000, Total Reward: -14, Epsilon: 0.05\n",
      "Episode 618/1000, Total Reward: -9, Epsilon: 0.05\n",
      "Episode 619/1000, Total Reward: -14, Epsilon: 0.05\n",
      "Episode 620/1000, Total Reward: -6, Epsilon: 0.05\n",
      "Episode 621/1000, Total Reward: -14, Epsilon: 0.05\n",
      "Episode 622/1000, Total Reward: -13, Epsilon: 0.04\n",
      "Episode 623/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 624/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 625/1000, Total Reward: -18, Epsilon: 0.04\n",
      "Episode 626/1000, Total Reward: -6, Epsilon: 0.04\n",
      "Episode 627/1000, Total Reward: -14, Epsilon: 0.04\n",
      "Episode 628/1000, Total Reward: -6, Epsilon: 0.04\n",
      "Episode 629/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 630/1000, Total Reward: 8, Epsilon: 0.04\n",
      "Episode 631/1000, Total Reward: -11, Epsilon: 0.04\n",
      "Episode 632/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 633/1000, Total Reward: -10, Epsilon: 0.04\n",
      "Episode 634/1000, Total Reward: -9, Epsilon: 0.04\n",
      "Episode 635/1000, Total Reward: -14, Epsilon: 0.04\n",
      "Episode 636/1000, Total Reward: 9, Epsilon: 0.04\n",
      "Episode 637/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 638/1000, Total Reward: 7, Epsilon: 0.04\n",
      "Episode 639/1000, Total Reward: -8, Epsilon: 0.04\n",
      "Episode 640/1000, Total Reward: -6, Epsilon: 0.04\n",
      "Episode 641/1000, Total Reward: -8, Epsilon: 0.04\n",
      "Episode 642/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 643/1000, Total Reward: -5, Epsilon: 0.04\n",
      "Episode 644/1000, Total Reward: -9, Epsilon: 0.04\n",
      "Episode 645/1000, Total Reward: -13, Epsilon: 0.04\n",
      "Episode 646/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 647/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 648/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 649/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 650/1000, Total Reward: -15, Epsilon: 0.04\n",
      "Episode 651/1000, Total Reward: -6, Epsilon: 0.04\n",
      "Episode 652/1000, Total Reward: -5, Epsilon: 0.04\n",
      "Episode 653/1000, Total Reward: 8, Epsilon: 0.04\n",
      "Episode 654/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 655/1000, Total Reward: 6, Epsilon: 0.04\n",
      "Episode 656/1000, Total Reward: -7, Epsilon: 0.04\n",
      "Episode 657/1000, Total Reward: -13, Epsilon: 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 658/1000, Total Reward: -10, Epsilon: 0.04\n",
      "Episode 659/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 660/1000, Total Reward: -14, Epsilon: 0.04\n",
      "Episode 661/1000, Total Reward: 9, Epsilon: 0.04\n",
      "Episode 662/1000, Total Reward: -13, Epsilon: 0.04\n",
      "Episode 663/1000, Total Reward: -12, Epsilon: 0.04\n",
      "Episode 664/1000, Total Reward: -13, Epsilon: 0.04\n",
      "Episode 665/1000, Total Reward: -10, Epsilon: 0.04\n",
      "Episode 666/1000, Total Reward: -14, Epsilon: 0.04\n",
      "Episode 667/1000, Total Reward: -11, Epsilon: 0.04\n",
      "Episode 668/1000, Total Reward: -9, Epsilon: 0.04\n",
      "Episode 669/1000, Total Reward: -5, Epsilon: 0.04\n",
      "Episode 670/1000, Total Reward: -14, Epsilon: 0.04\n",
      "Episode 671/1000, Total Reward: -16, Epsilon: 0.04\n",
      "Episode 672/1000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 673/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 674/1000, Total Reward: -12, Epsilon: 0.03\n",
      "Episode 675/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 676/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 677/1000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 678/1000, Total Reward: 5, Epsilon: 0.03\n",
      "Episode 679/1000, Total Reward: -11, Epsilon: 0.03\n",
      "Episode 680/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 681/1000, Total Reward: -12, Epsilon: 0.03\n",
      "Episode 682/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 683/1000, Total Reward: -11, Epsilon: 0.03\n",
      "Episode 684/1000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 685/1000, Total Reward: 6, Epsilon: 0.03\n",
      "Episode 686/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 687/1000, Total Reward: -10, Epsilon: 0.03\n",
      "Episode 688/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 689/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 690/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 691/1000, Total Reward: -12, Epsilon: 0.03\n",
      "Episode 692/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 693/1000, Total Reward: 4, Epsilon: 0.03\n",
      "Episode 694/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 695/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 696/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 697/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 698/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 699/1000, Total Reward: -8, Epsilon: 0.03\n",
      "Episode 700/1000, Total Reward: -13, Epsilon: 0.03\n",
      "Episode 701/1000, Total Reward: -8, Epsilon: 0.03\n",
      "Episode 702/1000, Total Reward: -13, Epsilon: 0.03\n",
      "Episode 703/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 704/1000, Total Reward: -11, Epsilon: 0.03\n",
      "Episode 705/1000, Total Reward: -14, Epsilon: 0.03\n",
      "Episode 706/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 707/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 708/1000, Total Reward: -14, Epsilon: 0.03\n",
      "Episode 709/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 710/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 711/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 712/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 713/1000, Total Reward: -6, Epsilon: 0.03\n",
      "Episode 714/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 715/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 716/1000, Total Reward: -8, Epsilon: 0.03\n",
      "Episode 717/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 718/1000, Total Reward: 4, Epsilon: 0.03\n",
      "Episode 719/1000, Total Reward: -10, Epsilon: 0.03\n",
      "Episode 720/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 721/1000, Total Reward: 5, Epsilon: 0.03\n",
      "Episode 722/1000, Total Reward: 5, Epsilon: 0.03\n",
      "Episode 723/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 724/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 725/1000, Total Reward: 8, Epsilon: 0.03\n",
      "Episode 726/1000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 727/1000, Total Reward: -13, Epsilon: 0.03\n",
      "Episode 728/1000, Total Reward: -15, Epsilon: 0.03\n",
      "Episode 729/1000, Total Reward: -10, Epsilon: 0.03\n",
      "Episode 730/1000, Total Reward: -12, Epsilon: 0.03\n",
      "Episode 731/1000, Total Reward: 10, Epsilon: 0.03\n",
      "Episode 732/1000, Total Reward: -11, Epsilon: 0.03\n",
      "Episode 733/1000, Total Reward: -7, Epsilon: 0.03\n",
      "Episode 734/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 735/1000, Total Reward: -5, Epsilon: 0.03\n",
      "Episode 736/1000, Total Reward: -11, Epsilon: 0.03\n",
      "Episode 737/1000, Total Reward: 8, Epsilon: 0.03\n",
      "Episode 738/1000, Total Reward: -9, Epsilon: 0.03\n",
      "Episode 739/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 740/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 741/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 742/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 743/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 744/1000, Total Reward: 10, Epsilon: 0.02\n",
      "Episode 745/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 746/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 747/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 748/1000, Total Reward: 9, Epsilon: 0.02\n",
      "Episode 749/1000, Total Reward: 3, Epsilon: 0.02\n",
      "Episode 750/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 751/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 752/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 753/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 754/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 755/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 756/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 757/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 758/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 759/1000, Total Reward: -13, Epsilon: 0.02\n",
      "Episode 760/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 761/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 762/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 763/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 764/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 765/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 766/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 767/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 768/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 769/1000, Total Reward: 10, Epsilon: 0.02\n",
      "Episode 770/1000, Total Reward: -14, Epsilon: 0.02\n",
      "Episode 771/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 772/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 773/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 774/1000, Total Reward: -14, Epsilon: 0.02\n",
      "Episode 775/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 776/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 777/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 778/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 779/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 780/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 781/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 782/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 783/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 784/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 785/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 786/1000, Total Reward: -14, Epsilon: 0.02\n",
      "Episode 787/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 788/1000, Total Reward: -1211, Epsilon: 0.02\n",
      "Episode 789/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 790/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 791/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 792/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 793/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 794/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 795/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 796/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 797/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 798/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 799/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 800/1000, Total Reward: 7, Epsilon: 0.02\n",
      "Episode 801/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 802/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 803/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 804/1000, Total Reward: -14, Epsilon: 0.02\n",
      "Episode 805/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 806/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 807/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 808/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 809/1000, Total Reward: -13, Epsilon: 0.02\n",
      "Episode 810/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 811/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 812/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 813/1000, Total Reward: -9, Epsilon: 0.02\n",
      "Episode 814/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 815/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 816/1000, Total Reward: 9, Epsilon: 0.02\n",
      "Episode 817/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 818/1000, Total Reward: -5, Epsilon: 0.02\n",
      "Episode 819/1000, Total Reward: 8, Epsilon: 0.02\n",
      "Episode 820/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 821/1000, Total Reward: -13, Epsilon: 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 822/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 823/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 824/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 825/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 826/1000, Total Reward: 9, Epsilon: 0.02\n",
      "Episode 827/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 828/1000, Total Reward: -10, Epsilon: 0.02\n",
      "Episode 829/1000, Total Reward: 8, Epsilon: 0.02\n",
      "Episode 830/1000, Total Reward: -8, Epsilon: 0.02\n",
      "Episode 831/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 832/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 833/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 834/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 835/1000, Total Reward: -7, Epsilon: 0.02\n",
      "Episode 836/1000, Total Reward: 3, Epsilon: 0.02\n",
      "Episode 837/1000, Total Reward: -11, Epsilon: 0.02\n",
      "Episode 838/1000, Total Reward: -6, Epsilon: 0.02\n",
      "Episode 839/1000, Total Reward: -12, Epsilon: 0.02\n",
      "Episode 840/1000, Total Reward: -14, Epsilon: 0.02\n",
      "Episode 841/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 842/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 843/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 844/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 845/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 846/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 847/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 848/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 849/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 850/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 851/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 852/1000, Total Reward: 7, Epsilon: 0.01\n",
      "Episode 853/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 854/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 855/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 856/1000, Total Reward: 6, Epsilon: 0.01\n",
      "Episode 857/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 858/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 859/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 860/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 861/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 862/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 863/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 864/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 865/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 866/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 867/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 868/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 869/1000, Total Reward: 9, Epsilon: 0.01\n",
      "Episode 870/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 871/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 872/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 873/1000, Total Reward: 9, Epsilon: 0.01\n",
      "Episode 874/1000, Total Reward: 8, Epsilon: 0.01\n",
      "Episode 875/1000, Total Reward: 5, Epsilon: 0.01\n",
      "Episode 876/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 877/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 878/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 879/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 880/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 881/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 882/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 883/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 884/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 885/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 886/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 887/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 888/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 889/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 890/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 891/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 892/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 893/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 894/1000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 895/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 896/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 897/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 898/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 899/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 900/1000, Total Reward: 9, Epsilon: 0.01\n",
      "Episode 901/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 902/1000, Total Reward: 4, Epsilon: 0.01\n",
      "Episode 903/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 904/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 905/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 906/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 907/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 908/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 909/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 910/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 911/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 912/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 913/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 914/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 915/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 916/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 917/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 918/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 919/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 920/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 921/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 922/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 923/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 924/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 925/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 926/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 927/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 928/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 929/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 930/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 931/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 932/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 933/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 934/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 935/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 936/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 937/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 938/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 939/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 940/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 941/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 942/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 943/1000, Total Reward: 4, Epsilon: 0.01\n",
      "Episode 944/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 945/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 946/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 947/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 948/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 949/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 950/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 951/1000, Total Reward: 3, Epsilon: 0.01\n",
      "Episode 952/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 953/1000, Total Reward: 9, Epsilon: 0.01\n",
      "Episode 954/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 955/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 956/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 957/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 958/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 959/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 960/1000, Total Reward: 5, Epsilon: 0.01\n",
      "Episode 961/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 962/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 963/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 964/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 965/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 966/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 967/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 968/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 969/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 970/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 971/1000, Total Reward: 7, Epsilon: 0.01\n",
      "Episode 972/1000, Total Reward: -7, Epsilon: 0.01\n",
      "Episode 973/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 974/1000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 975/1000, Total Reward: 6, Epsilon: 0.01\n",
      "Episode 976/1000, Total Reward: 5, Epsilon: 0.01\n",
      "Episode 977/1000, Total Reward: -13, Epsilon: 0.01\n",
      "Episode 978/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 979/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 980/1000, Total Reward: 6, Epsilon: 0.01\n",
      "Episode 981/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 982/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 983/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 984/1000, Total Reward: -5, Epsilon: 0.01\n",
      "Episode 985/1000, Total Reward: -7, Epsilon: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 986/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 987/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 988/1000, Total Reward: -10, Epsilon: 0.01\n",
      "Episode 989/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 990/1000, Total Reward: -11, Epsilon: 0.01\n",
      "Episode 991/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 992/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 993/1000, Total Reward: -9, Epsilon: 0.01\n",
      "Episode 994/1000, Total Reward: -6, Epsilon: 0.01\n",
      "Episode 995/1000, Total Reward: -12, Epsilon: 0.01\n",
      "Episode 996/1000, Total Reward: -14, Epsilon: 0.01\n",
      "Episode 997/1000, Total Reward: 10, Epsilon: 0.01\n",
      "Episode 998/1000, Total Reward: -8, Epsilon: 0.01\n",
      "Episode 999/1000, Total Reward: 8, Epsilon: 0.01\n",
      "Episode 1000/1000, Total Reward: -5, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RailEnvironment(grid_size=(10, 10), n_agents=5)  # Single agent for simplicity\n",
    "    agent = DQLAgent(state_size=env.grid.size, action_size=5)\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset().flatten()  # Flatten grid to vector\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, rewards, done = env.step([action])\n",
    "            next_state = next_state.flatten()\n",
    "            reward = sum(rewards.values())\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.replay(batch_size)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4713955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
